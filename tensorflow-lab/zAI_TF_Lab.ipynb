{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "### https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Let's import the packages we will use in our lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math, time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pydot\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import joblib\n",
    "print(\"Using TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "tf.random.set_seed(5678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Let's load the order quantity dataset that we will use to train and test our LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./data_550k.csv\")\n",
    "print('Number of rows:', data.shape[0])\n",
    "print('Number of columns:', data.shape[1])\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Let's take a look at the distribution of the dataset with some descriptive statistics and plot some of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(data.values)\n",
    "plt.title('Actual order quantity vs unit time')\n",
    "plt.xlim(0, 1000)\n",
    "plt.xlabel('Unit Time')\n",
    "plt.ylabel('Order quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Let's define the time step, which is the number of historical order quantities we will use to predict the next order quantity.  Since our time step is 20, we will use the past 20 orders to predict the next order.  Since we are predicting order quantity, we will have 1 feature and 1 label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20\n",
    "feature_size = 1\n",
    "label_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Let's split the input dataset into training and testing datasets with a 70:30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data).flatten()\n",
    "train_size = int(len(data) * 0.70)\n",
    "test_size = int(len(data) * 0.30)\n",
    "train  = data[0:train_size]\n",
    "test = data[train_size:len(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6: Let's create our features (X) and labels (y) from our training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(len(train)-time_steps):\n",
    "    for j in range(time_steps):\n",
    "        X_train.append(train[i+j])\n",
    "y_train = []\n",
    "for i in range(len(train)-time_steps):\n",
    "    y_train.append(train[i + time_steps])\n",
    "X_test = []\n",
    "for i in range(len(test)-time_steps):\n",
    "    for j in range(time_steps):\n",
    "        X_test.append(test[i+j])\n",
    "y_test = []\n",
    "for i in range(len(test)-time_steps):\n",
    "    y_test.append(test[i + time_steps])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 7: Let's print out a few values of our training dataset and see how the training features (X_train) and training labels (y_train) are grouped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X_train[0:20])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8: Let's also print out a few values of our testing dataset and see how the testing features (X_test) and testing labels (y_test) are grouped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X_test[0:20])\n",
    "print(y_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9: Let's normalize the training dataset within a range of 0 to 1.  We will use the slope and intercept of the fitted MinMaxScaler to scale and unscale the data.  We will do this with custom Lambda layers in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Xscaled = scaler.fit_transform(pd.DataFrame(train))\n",
    "print(Xscaled)\n",
    "scalerSlope = scaler.scale_[0]\n",
    "scalerIntercept = scaler.min_[0]\n",
    "print(\"slope = \", scalerSlope)\n",
    "print(\"intercept = \", scalerIntercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X):    \n",
    "    X = (X * scalerSlope) + scalerIntercept\n",
    "    return X\n",
    "\n",
    "def scaleInv(X):\n",
    "    X = (X - scalerIntercept) / scalerSlope\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 10: Let's reshape our data to fit the format of an LSTM network.  This will be (number of samples, time_steps, feature_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, ((len(train)-time_steps), time_steps, feature_size))\n",
    "y_train = np.reshape(y_train, ((len(train)-time_steps), label_size))\n",
    "X_test = np.reshape(X_test, ((len(test)-time_steps), time_steps, feature_size))\n",
    "y_test = np.reshape(y_test, ((len(test)-time_steps), label_size))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 11: Let's define the parameters for our LSTM network.  Our network will be built with 50 cells.  We will use a batch size of 64 to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM parameters\n",
    "cells = 50\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 12: Let's build our LSTM network.  We will use the Keras framework to define a 2-layer LSTM.  Since we are using the LSTM for a regression analysis, we will use a linear activation function and mean squared error for our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_input = (time_steps, feature_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([    \n",
    "    tf.keras.layers.Lambda(scale),\n",
    "    tf.keras.layers.LSTM(cells, input_shape=tf_input, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(cells),\n",
    "    tf.keras.layers.Dense(label_size, activation='linear'),\n",
    "    tf.keras.layers.Lambda(scaleInv)])\n",
    "    \n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 13: Let's train our network.  We will train for 1 epoch, or 1 iteration through the training dataset, to save time in the lab.  Try experimenting with more epochs and notice how the loss and mean squared error decrease after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training...\")\n",
    "model.fit(X_train.astype(\"float64\"), y_train, epochs = 1, batch_size = batch_size, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, save_dir+\"/model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 14: Let's plot the first 250 predicted order quantities and compare our results to the actual order quantities.  The values in the left column are the actual 21st order quantities for the first 15 groups of sequences.  The values in the right column represent the model's prediction of the 21st order quantities for the first 15 groups of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_order_quantity = model.predict(X_test[0:250]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"y_test predicted_order_quantity\")\n",
    "for i in range(15):\n",
    "    print(y_test[i], predicted_order_quantity[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(y_test, color = 'red', label = 'Actual order quantity')\n",
    "plt.plot(predicted_order_quantity, color = 'blue', label = 'Predicted order quantity')\n",
    "plt.xticks(np.arange(0,len(X_test),50))\n",
    "plt.title('Actual order quantity vs prediction')\n",
    "plt.xlim(0, 250)\n",
    "plt.xlabel('Sequence')\n",
    "plt.ylabel('Order quantity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 15: Let's plot the difference between our predicted order quantities and the actual order quantities.  As you can see, the difference is mostly within +/-5 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(y_test[0:250] - predicted_order_quantity)\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(-50,51)\n",
    "plt.yticks(np.arange(-50, 51, 5))\n",
    "plt.axhline(y=-5, color='r')\n",
    "plt.axhline(y=5, color='r')\n",
    "plt.title('Difference between predicted and actual order quantities')\n",
    "plt.xlabel('Sequence')\n",
    "plt.ylabel('Range')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 16: Let's compute the percentage of occurences with a difference between predicted order quantities and actual order quantities of less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff = np.absolute(y_test[0:250] - predicted_order_quantity)\n",
    "print('{:5.0f}%'.format((diff < 5).sum() / diff.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 17: Let's save our model, restore it, and re-run the predictions to verify our results are the same.  Again, the values in the left column are the actual 21st order quantities for the first 15 groups of sequences.  The values in the right column represent the model's prediction of the 21st order quantities for the first 15 groups of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(os.path.join(save_dir+\"/1\",\"wts\"))\n",
    "model.save(save_dir+\"/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_model = tf.keras.models.load_model(save_dir+\"/1\")\n",
    "restored_model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['mean_squared_error'])\n",
    "restored_predicted_order_quantity = restored_model.predict(X_test[0:250]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_test restored_predicted_order_quantity\")\n",
    "for i in range(15):\n",
    "    print(y_test[i], restored_predicted_order_quantity[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 18: Let's work with our model in TensorFlow Serving and re-run the predictions to verify our results are the same.  We need to start the Tensorflow Serving process in the background before running the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cmd = \"tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=saved_model --model_base_path=/workspace/tf-model-dev-lab/saved_model\"\n",
    "subprocess.Popen(cmd.split(), close_fds=True)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = json.dumps({\"instances\": X_test[0:15].tolist()})\n",
    "json_response = requests.post('http://localhost:8501/v1/models/saved_model:predict', data=data)\n",
    "result = np.array(json.loads(json_response.text)[\"predictions\"]).astype(int)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 19: Let's try to manually enter a sequence of 20 order quantities and let the model predict the next quantity.  Copy the following line of code and paste in the next cell.  Replace the #s with integers:\n",
    "\n",
    "X_input = np.array([[#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#], [#]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = np.array([[71],\n",
    "       [71],\n",
    "       [71],\n",
    "       [41],\n",
    "       [71],\n",
    "       [71],\n",
    "       [32],\n",
    "       [71],\n",
    "       [70],\n",
    "       [70],\n",
    "       [25],\n",
    "       [50],\n",
    "       [45],\n",
    "       [71],\n",
    "       [13],\n",
    "       [71],\n",
    "       [20],\n",
    "       [71],\n",
    "       [71],\n",
    "       [69]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = np.reshape(X_input, (-1, X_input.shape[0], feature_size))\n",
    "print(X_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = json.dumps({\"instances\": X_input.tolist()})\n",
    "json_response = requests.post('http://localhost:8501/v1/models/saved_model:predict', data=data_input)\n",
    "result = np.array(json.loads(json_response.text)[\"predictions\"]).astype(int)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
